import React, { useState, useRef, useEffect } from 'react';
import { FaceDetector, FilesetResolver } from '@mediapipe/tasks-vision';

const containerStyle = { maxWidth: '600px', margin: '0 auto', padding: '20px', fontFamily: '-apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif' };
const buttonStyle = { padding: '12px 24px', margin: '5px', background: '#007bff', color: '#fff', border: 'none', borderRadius: '8px', fontSize: '16px', fontWeight: 'bold', cursor: 'pointer' };

function App() {
  const [fileExt, setFileExt] = useState("webm"); 
  const [detector, setDetector] = useState(null);
  const [videoSrc, setVideoSrc] = useState(null);
  const [maskSrc, setMaskSrc] = useState(null);
  const [isProcessing, setIsProcessing] = useState(false);
  const [status, setStatus] = useState("æ­£åœ¨åŠ è½½ AI æ¨¡å‹...");
  const [downloadUrl, setDownloadUrl] = useState(null);

  const videoRef = useRef(null);
  const canvasRef = useRef(null);
  const maskImgRef = useRef(null);
  const chunksRef = useRef([]);
  const lastFaceRef = useRef(null);
  
  // ğŸ”¥ æ–°å¢ï¼šè®°å½•äººè„¸ç§»åŠ¨é€Ÿåº¦ï¼Œç”¨äºé¢„æµ‹
  const velocityRef = useRef({ x: 0, y: 0 });

  const audioCtxRef = useRef(null);
  const sourceNodeRef = useRef(null);
  const destNodeRef = useRef(null);

  const initLockRef = useRef(false);

  useEffect(() => {
    if (initLockRef.current === true) return;
    initLockRef.current = true;

    const initAI = async () => {
      console.log("ğŸš€ å¼€å§‹åˆå§‹åŒ– AI æ¨¡å‹...");
      try {
        const vision = await FilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
        );
        const faceDetector = await FaceDetector.createFromOptions(vision, {
          baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite`,
            delegate: "GPU"
          },
          runningMode: "VIDEO",
          // ğŸ”¥ é™ä½é—¨æ§›ï¼šè®©æ¨¡ç³Šçš„è„¸ä¹Ÿèƒ½è¢«è¯†åˆ«åˆ°
          minDetectionConfidence: 0.3, 
          minSuppressionThreshold: 0.3 
        });
        
        setDetector(faceDetector);
        setStatus("âœ… AI å°±ç»ªï¼è¯·å¯¼å…¥è§†é¢‘");
      } catch (err) {
        setStatus(`âŒ æ¨¡å‹åŠ è½½å¤±è´¥: ${err.message}`);
        console.error(err);
      }
    };
    initAI();
  }, []);

  const handleVideoUpload = (e) => {
    const file = e.target.files[0];
    if (file) {
      setVideoSrc(URL.createObjectURL(file));
      setDownloadUrl(null);
      setStatus("è§†é¢‘å·²åŠ è½½");
    }
  };

  const handleMaskUpload = (e) => {
    const file = e.target.files[0];
    if (file) {
      const img = new Image();
      img.src = URL.createObjectURL(file);
      img.onload = () => { maskImgRef.current = img; };
      setMaskSrc(img.src);
    }
  };

  const startProcessing = async () => {
    if (!detector || !videoRef.current || !maskImgRef.current) {
      alert("è¯·ç¡®ä¿è§†é¢‘ã€é®æŒ¡å›¾å’ŒAIæ¨¡å‹éƒ½å·²å°±ç»ª");
      return;
    }

    const video = videoRef.current;
    
    if (video.readyState < 2) {
        setStatus("æ­£åœ¨ç¼“å†²è§†é¢‘...");
        await new Promise(resolve => {
            video.onloadeddata = resolve;
            setTimeout(resolve, 1500); 
        });
    }

    setIsProcessing(true);
    setStatus("æ­£åœ¨åˆå§‹åŒ–...");
    setDownloadUrl(null);
    chunksRef.current = [];
    lastFaceRef.current = null;
    velocityRef.current = { x: 0, y: 0 }; // é‡ç½®é€Ÿåº¦

    const canvas = canvasRef.current;
    canvas.width = video.videoWidth || 640;
    canvas.height = video.videoHeight || 480;
    const ctx = canvas.getContext('2d');

    // === éŸ³é¢‘å¤„ç† ===
    try {
        if (!audioCtxRef.current) {
            audioCtxRef.current = new (window.AudioContext || window.webkitAudioContext)();
        }
        const audioCtx = audioCtxRef.current;
        if (audioCtx.state === 'suspended') await audioCtx.resume();

        if (sourceNodeRef.current) {
            try { sourceNodeRef.current.disconnect(); } catch(e){}
        }
        sourceNodeRef.current = audioCtx.createMediaElementSource(video);
        if (!destNodeRef.current) destNodeRef.current = audioCtx.createMediaStreamDestination();
        sourceNodeRef.current.connect(destNodeRef.current);
    } catch (e) {
        console.warn("éŸ³é¢‘åˆå§‹åŒ–è­¦å‘Š:", e);
    }

    // === æ··åˆæµ ===
    const canvasStream = canvas.captureStream(30); 
    if (destNodeRef.current) {
        const audioTrack = destNodeRef.current.stream.getAudioTracks()[0];
        if (audioTrack) canvasStream.addTrack(audioTrack);
    }

    // === æ ¼å¼é€‰æ‹© ===
    const options = [
        { mimeType: 'video/webm; codecs=vp9', ext: 'webm' },
        { mimeType: 'video/webm', ext: 'webm' },
        { mimeType: 'video/mp4; codecs="avc1.42E01E, mp4a.40.2"', ext: 'mp4' },
        { mimeType: 'video/mp4', ext: 'mp4' }
    ];

    let selectedOption = options.find(opt => MediaRecorder.isTypeSupported(opt.mimeType));
    
    if (!selectedOption) {
        selectedOption = { mimeType: '', ext: 'webm' };
    }

    setFileExt(selectedOption.ext);

    let recorder;
    try {
        recorder = new MediaRecorder(canvasStream, { 
            mimeType: selectedOption.mimeType,
            videoBitsPerSecond: 2500000 
        });
    } catch (e) {
        recorder = new MediaRecorder(canvasStream);
    }
    
    recorder.ondataavailable = (e) => {
      if (e.data && e.data.size > 0) {
          chunksRef.current.push(e.data);
      }
    };

    recorder.onstop = () => {
      const blob = new Blob(chunksRef.current, { type: selectedOption.mimeType || 'video/webm' });
      
      if (blob.size === 0) {
          alert("ç”Ÿæˆå¤±è´¥ï¼šæ–‡ä»¶å¤§å°ä¸º0ã€‚");
          setIsProcessing(false);
          return;
      }

      const url = URL.createObjectURL(blob);
      setDownloadUrl(url);
      setIsProcessing(false);
      setStatus("âœ… å¤„ç†å®Œæˆï¼");
      
      video.pause();
      video.muted = false;
      canvasStream.getTracks().forEach(track => track.stop());
    };

    recorder.start(100); 
    
    try {
        video.currentTime = 0;
        video.muted = false; 
        await video.play();
        processFrame(video, ctx, recorder);
    } catch (e) {
        console.error("æ’­æ”¾å¤±è´¥:", e);
        setStatus(`æ’­æ”¾é”™è¯¯: ${e.message}`);
        setIsProcessing(false);
        recorder.stop();
    }
  };

  const processFrame = (video, ctx, recorder) => {
    if (video.ended || video.paused) {
      if (recorder.state === 'recording') recorder.stop();
      return;
    }

    const canvas = ctx.canvas;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    const startTimeMs = performance.now();
    let detections = null;
    try {
        if (detector) {
            detections = detector.detectForVideo(video, startTimeMs).detections;
        }
    } catch(e) { console.error(e); }

    let targetFace = null;

    if (detections && detections.length > 0) {
      // 1. æˆåŠŸæ£€æµ‹åˆ°äººè„¸
      const face = detections[0].boundingBox;
      
      // ğŸ”¥ è®¡ç®—ç§»åŠ¨é€Ÿåº¦ (æƒ¯æ€§)
      if (lastFaceRef.current) {
         const vx = face.originX - lastFaceRef.current.originX;
         const vy = face.originY - lastFaceRef.current.originY;
         // æ›´æ–°é€Ÿåº¦
         velocityRef.current = { x: vx, y: vy };
      }
      
      targetFace = face;
      lastFaceRef.current = face;

    } else if (lastFaceRef.current) {
      // 2. è·Ÿä¸¢äº†ï¼å¯åŠ¨â€œæƒ¯æ€§é¢„æµ‹â€æ¨¡å¼
      // ä¸å†åªæ˜¯åœç•™åœ¨åŸåœ°ï¼Œè€Œæ˜¯æ ¹æ®æœ€åçš„é€Ÿåº¦ç»§ç»­â€œé£â€
      const vx = velocityRef.current.x;
      const vy = velocityRef.current.y;
      
      const predictedFace = {
          ...lastFaceRef.current,
          originX: lastFaceRef.current.originX + vx,
          originY: lastFaceRef.current.originY + vy,
          width: lastFaceRef.current.width,
          height: lastFaceRef.current.height
      };
      
      // æ…¢æ…¢å‡é€Ÿï¼ˆæ‘©æ“¦åŠ›ï¼‰ï¼Œé˜²æ­¢é¢„æµ‹è¿‡å¤´é£å‡ºå±å¹•
      velocityRef.current = { x: vx * 0.9, y: vy * 0.9 };
      
      targetFace = predictedFace;
      // æ›´æ–°ä½ç½®ï¼Œè¿™æ ·ä¸‹ä¸€å¸§å¦‚æœè¿˜ä¸¢äº†ï¼Œå°±ä¼šåŸºäºè¿™ä¸ªé¢„æµ‹ä½ç½®ç»§ç»­é£
      lastFaceRef.current = predictedFace; 
    }

    if (targetFace && maskImgRef.current) {
      const { originX, originY, width, height } = targetFace;
      
      // ğŸ”¥ æ‰©å¤§é®æŒ¡èŒƒå›´ï¼šä» 1.3 å€å¢åŠ åˆ° 1.5 å€ï¼Œå®å¯å¤šé®ä¸èƒ½æ¼
      const scale = 1.5; 
      const w = width * scale;
      const h = height * scale;
      const x = originX - (w - width) / 2;
      const y = originY - (h - height) / 2;
      
      ctx.drawImage(maskImgRef.current, x, y, w, h);
    }

    requestAnimationFrame(() => processFrame(video, ctx, recorder));
  };

  return (
    <div style={containerStyle}>
      <h2 style={{textAlign: 'center'}}>ä¿æŠ¤è±†ç§(å¼ºåŠ›è¿½è¸ªç‰ˆ)</h2>
      <p style={{textAlign: 'center', color: isProcessing ? '#d9534f' : '#666', fontWeight: isProcessing ? 'bold' : 'normal'}}>
        {status}
      </p>

      <div style={{background: '#f8f9fa', padding: '15px', borderRadius: '10px', marginBottom: '20px', boxShadow: '0 2px 5px rgba(0,0,0,0.05)'}}>
        <div style={{marginBottom: '15px'}}>
            <label style={{display: 'block', fontWeight: 'bold', marginBottom: '8px'}}>1. å¯¼å…¥è§†é¢‘ ğŸ“¹</label>
            <input type="file" accept="video/*" onChange={handleVideoUpload} style={{width: '100%'}} />
        </div>
        
        <div>
            <label style={{display: 'block', fontWeight: 'bold', marginBottom: '8px'}}>2. å¯¼å…¥é®æŒ¡è¡¨æƒ… ğŸƒ</label>
            <input type="file" accept="image/*" onChange={handleMaskUpload} style={{width: '100%'}} />
        </div>
      </div>

      <video 
        ref={videoRef} 
        src={videoSrc} 
        style={{ position: 'fixed', top: 0, left: 0, opacity: 0, pointerEvents: 'none', zIndex: -1, width: '1px', height: '1px' }} 
        playsInline 
        webkit-playsinline="true"
        crossOrigin="anonymous"
      />

      <div style={{ 
          border: '2px solid #333', 
          borderRadius: '8px',
          background: '#000', 
          minHeight: '200px', 
          display: 'flex', 
          justifyContent: 'center', 
          alignItems: 'center',
          overflow: 'hidden',
          marginBottom: '20px'
      }}>
        <canvas ref={canvasRef} style={{ maxWidth: '100%', maxHeight: '60vh', display: 'block' }} />
      </div>

      <div style={{ display: 'flex', justifyContent: 'center', gap: '15px', flexWrap: 'wrap' }}>
        <button 
          style={{...buttonStyle, opacity: (isProcessing || !videoSrc) ? 0.6 : 1, width: '100%'}} 
          onClick={startProcessing} 
          disabled={isProcessing || !videoSrc}
        >
          {isProcessing ? 'â³ å¤„ç†ä¸­ (AIæ­£åœ¨ç©å‘½è¿½è¸ª)...' : 'ğŸš€ å¼€å§‹ç”Ÿæˆ'}
        </button>

        {downloadUrl && (
          <a 
            href={downloadUrl} 
            download={`masked_video_${Date.now()}.${fileExt}`}
            style={{...buttonStyle, background: '#28a745', textDecoration: 'none', display: 'flex', alignItems: 'center', justifyContent: 'center', width: '100%'}}
          >
            ğŸ’¾ ä¿å­˜åˆ°ç›¸å†Œ ({fileExt.toUpperCase()})
          </a>
        )}
      </div>
    </div>
  );
}

export default App;